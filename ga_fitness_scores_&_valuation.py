# -*- coding: utf-8 -*-
"""GA_Fitness_Scores_&_Valuation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1z9vX0vs9P30iVYlMyrWkEPr-ngvyiNxw
"""

# Mount google drive at /content/drive
from google.colab import drive
drive.mount('/content/drive')

!pip uninstall -y scikit-learn
!pip install scikit-learn==1.3.1
!pip install sklearn-genetic-opt

# Set seeds
import numpy as np
import tensorflow as tf
from sklearn.preprocessing import StandardScaler
import json
import pandas as pd
np.random.seed(42)
tf.random.set_seed(42)

# Read the contents of the first file
with open('/content/drive/MyDrive/BDDF_Research/approved_updated_proteins.txt', 'r') as file1:
    content1 = file1.read().splitlines()

# Read the contents of the second file
with open('/content/drive/MyDrive/BDDF_Research/investigational_proteins.txt', 'r') as file2:
    content2 = file2.read().splitlines()

# Combine the contents of both files and remove duplicates
merged_content = sorted(set(content1 + content2))

# Write the merged content to a new file
with open('merged.txt', 'w') as merged_file:
    for line in merged_content:
        merged_file.write(line + '\n')

print("Files merged successfully into 'merged.txt'.")

# Fetching PCP properties of druggable and non-druggable proteins
data_file_path = "/content/drive/MyDrive/protein_props.json"
druggable_proteins_file_path = "/content/drive/MyDrive/BDDF_Research/merged.txt"
approved_druggable_proteins_file_path = "/content/drive/MyDrive/BDDF_Research/approved_updated_proteins.txt"

with open(data_file_path, 'r') as f:
    protein_data = json.load(f)

print("Total number of uniprot human verified proteins:", len(protein_data))

with open(druggable_proteins_file_path, 'r') as f:
    druggable_proteins = f.read().splitlines()

with open(approved_druggable_proteins_file_path, 'r') as f:
    approved_druggable_proteins = f.read().splitlines()

print("Number of druggable proteins:", len(druggable_proteins))
print("Number of approved druggable proteins:", len(approved_druggable_proteins))

properties = (pd.read_json("/content/drive/MyDrive/protein_props.json")).transpose()
is_druggable = [1 if i in druggable_proteins else 0 for i in properties.index]
is_approved_druggable = [1 if i in approved_druggable_proteins else 0 for i in properties.index]

properties["is_druggable"] = is_druggable
properties["is_approved_druggable"] = is_approved_druggable

PCP_properties = properties.copy()
amino_acids = 'ACDEFGHIKLMNPQRSTVWY'
amino_acid_percent = {i:[] for i in amino_acids}
for i in PCP_properties['Amino Acid Percent']:
  for aa in amino_acids:
    amino_acid_percent[aa].append(i[aa])
for aa in amino_acids:
  PCP_properties = pd.concat([PCP_properties, pd.Series(amino_acid_percent[aa], index = PCP_properties.index, name = f"Amino Acid Percent {aa}")], axis = 1)

PCP_properties[f"Molar Extinction Coefficient 1"] = pd.Series([x[0] for x in PCP_properties['Molar Extinction Coefficient']], index = PCP_properties.index)
PCP_properties[f"Molar Extinction Coefficient 2"] = pd.Series([x[1] for x in PCP_properties['Molar Extinction Coefficient']], index = PCP_properties.index)

PCP_properties[f"Secondary Structure helix"] = pd.Series([x[0] for x in PCP_properties['Secondary Structure']], index = PCP_properties.index)
PCP_properties[f"Secondary Structure turn"] = pd.Series([x[1] for x in PCP_properties['Secondary Structure']], index = PCP_properties.index)
PCP_properties[f"Secondary Structure sheet"] = pd.Series([x[2] for x in PCP_properties['Secondary Structure']], index = PCP_properties.index)

PCP_properties.drop(columns = ['Amino Acid Count','Amino Acid Percent',"Molar Extinction Coefficient","Flexibility","Secondary Structure",'Sequence'], inplace = True)
PCP_properties['Sequence Length'] = PCP_properties['Sequence Length'].astype(int)
PCP_properties[['Molecular Weight', 'GRAVY', 'Isoelectric Point', 'Instability Index', 'Aromaticity', 'Charge at 7']] = PCP_properties[['Molecular Weight', 'GRAVY', 'Isoelectric Point', 'Instability Index', 'Aromaticity', 'Charge at 7']].astype(float)

with open("/content/drive/MyDrive/BDDF_Research/gdpc_encodings.json", 'r') as file:
    data = json.load(file)
gpdc_encodings = pd.DataFrame(data).transpose()

ppi = pd.read_json("/content/drive/MyDrive/ppi.json").transpose()
ppi_network = pd.read_csv("/content/drive/MyDrive/BDDF_Research/ppi_network_properties.csv")
ppi_network.index = ppi_network['Unnamed: 0']
ppi_network.drop(columns = ['Unnamed: 0'], inplace = True)
ppi = pd.concat([ppi, ppi_network], axis = 1)

glycolisation = pd.read_csv("/content/drive/MyDrive/glycosylation.csv")
glycolisation.index = glycolisation['Unnamed: 0']
glycolisation.drop(columns = ['Unnamed: 0'], inplace = True)
ptm = pd.read_csv("/content/drive/MyDrive/PTM_counts.csv")
ptm.index = ptm["Unnamed: 0"]
ptm.drop(columns = ['Unnamed: 0'], inplace = True)
ptm_counts = pd.concat([ptm, glycolisation], axis = 1)

with open("/content/drive/MyDrive/subcellular_locations2.json", 'r') as file:
    data = json.load(file)
unique_groups = set()
for entry in data.values():
    if "general" in entry:
        for general_entry in entry["general"]:
            if "group" in general_entry: unique_groups.add(general_entry["group"])

unique_groups_list = list(unique_groups)

rows = []
for protein_id in PCP_properties.index:
    row = {group: 0 for group in unique_groups_list}
    if protein_id in data:
        for entry in data[protein_id].get("general", []):
            if "group" in entry and entry["group"] in unique_groups:
                row[entry["group"]] = 1
    row["protein_id"] = protein_id
    rows.append(row)

subcellular_data = pd.DataFrame(rows).set_index("protein_id")

domains = pd.read_csv("/content/drive/MyDrive/BDDF_Research/data_top20_updated.csv")
domains.index = domains['Unnamed: 0']
domains.drop(columns = ['Unnamed: 0'], inplace = True)

flexibility = pd.read_csv("/content/drive/MyDrive/BDDF_Research/flexibility_properties.csv")
flexibility.index = flexibility['Unnamed: 0']
flexibility.drop(columns = ['Unnamed: 0'], inplace = True)

latent_data = pd.read_csv("/content/drive/MyDrive/BDDF_Research/latent_values.csv").transpose()
latent_data.columns = [f"Latent_Value_{i+1}" for i in latent_data.columns]
final_data = pd.concat([PCP_properties,gpdc_encodings, ptm_counts, ppi, subcellular_data, domains, flexibility, latent_data], axis = 1).dropna()
features_list = final_data.columns
y = final_data['is_approved_druggable']
features_list = features_list.drop(['is_druggable','is_approved_druggable'])
features_list = list(features_list)
print(features_list)
print(len(features_list))

#for splitting of data
from sklearn.model_selection import train_test_split
from sklearn.utils import shuffle
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from imblearn.over_sampling import ADASYN, SMOTE

def get_data(x_sample, y_sample):
  return np.array(x_sample), np.array(y_sample)

def data_splitting(x_sample, y_sample, mode="default", scaler="none", class_size=600):
  druggable_indices = (y_sample == 1)  # Assuming 1 represents druggable
  non_druggable_indices = (y_sample == 0)  # Assuming 0 represents non-druggable

  druggable_X = x_sample[druggable_indices]
  druggable_y = y_sample[druggable_indices]

  non_druggable_X = x_sample[non_druggable_indices]
  non_druggable_y = y_sample[non_druggable_indices]

  druggable_X_remaining, druggable_X_test, druggable_y_remaining, druggable_y_test = train_test_split(druggable_X, druggable_y, test_size=class_size, random_state=123)
  non_druggable_X_remaining, non_druggable_X_test, non_druggable_y_remaining, non_druggable_y_test = train_test_split(non_druggable_X, non_druggable_y, test_size= class_size, random_state=123)

  X_test = pd.concat((druggable_X_test, non_druggable_X_test))
  y_test = pd.concat((druggable_y_test, non_druggable_y_test))
  X_train = pd.concat((druggable_X_remaining, non_druggable_X_remaining))
  y_train = pd.concat((druggable_y_remaining, non_druggable_y_remaining))
  X_train, y_train = shuffle(X_train, y_train, random_state=123)
  if mode == "default":
    pass
  elif mode == "adasyn":
    ada = ADASYN(random_state=42)
    X_train, y_train = ada.fit_resample(X_train, y_train)
  elif mode == "smote":
    smt = SMOTE(random_state=42)
    X_train, y_train = smt.fit_resample(X_train, y_train)

  if scaler == "std":
    scaler = StandardScaler()
    X_train = scaler.fit_transform(X_train)
    X_test = scaler.transform(X_test)
  elif scaler == "minmax":
    scaler = MinMaxScaler()
    X_train = scaler.fit_transform(X_train)
    X_test = scaler.transform(X_test)
  elif scaler == "none":
    pass

  return X_train, X_test, y_train, y_test

non_approved_druggable = new_data[new_data["new_column"] == 1]
X_tests, y_tests = get_data(non_approved_druggable[features_list], non_approved_druggable["is_approved_druggable"])
protein_names = non_approved_druggable.index

y_preds = []
y_probs = []
for i,model in enumerate(models_list):
  y_pred = model.predict(np.asarray(X_tests)[:, okk[i]])
  y_preds.append(y_pred)
  y_prob = model.predict_proba(np.asarray(X_tests)[:, okk[i]])[:,1]
  y_probs.append(y_prob)

majority_preds = np.mean(y_preds, axis=0)
majority_preds = np.round(majority_preds)

majority_probs = np.mean(y_probs, axis=0)

data = {
    "Protein": protein_names
}
for i,probs in enumerate(y_probs):
  data[f"Probability_Partition_{i+1}"] = probs
  data["Mean_Probability"] = majority_probs
  data["Majority_Prediction"] = majority_preds

import matplotlib.pyplot as plt
import numpy as np

# Fitness values for each generation as given in the output
generations = np.array([0, 1, 2, 3, 4, 5])

fitness_max_0 = np.array([0.743403, 0.745526, 0.749469, 0.749469, 0.749469, 0.749469])
fitness_min_0 = np.array([0.709736, 0.716409, 0.713376, 0.723082, 0.711556, 0.716409])

fitness_max_1 = np.array([0.741583, 0.738854, 0.734911, 0.744313, 0.744313, 0.74401])
fitness_min_1 = np.array([0.690021, 0.699727, 0.702457, 0.719442, 0.719745, 0.709433])

fitness_max_2 = np.array([0.740977, 0.7431, 0.745526, 0.745526, 0.744616, 0.746436])
fitness_min_2 = np.array([0.689111, 0.713982, 0.716712, 0.70731, 0.710646, 0.710646])

fitness_max_3 = np.array([0.753412, 0.745223, 0.752199, 0.74583, 0.760388, 0.760388])
fitness_min_3 = np.array([0.7064, 0.706703, 0.728844, 0.720958, 0.721262, 0.724598])

fitness_max_4 = np.array([0.747953, 0.747953, 0.7431, 0.7431, 0.7431, 0.7431])
fitness_min_4 = np.array([0.685472, 0.702457, 0.721262, 0.708523, 0.719139, 0.713072])

fitness_max_5 = np.array([0.750076, 0.750076, 0.750076, 0.752806, 0.752502, 0.750076])
fitness_min_5 = np.array([0.695784, 0.722172, 0.722172, 0.70003, 0.716106, 0.709736])

fitness_max_6 = np.array([0.740673, 0.748559, 0.748559, 0.74401, 0.74583, 0.74583])
fitness_min_6 = np.array([0.701244, 0.712163, 0.719442, 0.718229, 0.734304, 0.737034])

fitness_max_7 = np.array([0.74181, 0.744847, 0.745759, 0.745759, 0.754253, 0.754253])
fitness_min_7 = np.array([0.692361, 0.711469, 0.718754, 0.706011, 0.712685, 0.715107])

fitness_max_8 = np.array([0.750001, 0.750001, 0.752432, 0.749397, 0.749397, 0.753036])
fitness_min_8 = np.array([0.700855, 0.721482, 0.726945, 0.715114, 0.706617, 0.713292])


# Calculate the average fitness and variance for each partition
fitness_avg_0 = (fitness_max_0 + fitness_min_0) / 2
fitness_avg_1 = (fitness_max_1 + fitness_min_1) / 2
fitness_avg_2 = (fitness_max_2 + fitness_min_2) / 2
fitness_avg_3 = (fitness_max_3 + fitness_min_3) / 2
fitness_avg_4 = (fitness_max_4 + fitness_min_4) / 2
fitness_avg_5 = (fitness_max_5 + fitness_min_5) / 2
fitness_avg_6 = (fitness_max_6 + fitness_min_6) / 2
fitness_avg_7 = (fitness_max_7 + fitness_min_7) / 2
fitness_avg_8 = (fitness_max_8 + fitness_min_8) / 2

# Calculate the variance (standard deviation) for each partition
fitness_std_0 = (fitness_max_0 - fitness_min_0) / 2
fitness_std_1 = (fitness_max_1 - fitness_min_1) / 2
fitness_std_2 = (fitness_max_2 - fitness_min_2) / 2
fitness_std_3 = (fitness_max_3 - fitness_min_3) / 2
fitness_std_4 = (fitness_max_4 - fitness_min_4) / 2
fitness_std_5 = (fitness_max_5 - fitness_min_5) / 2
fitness_std_6 = (fitness_max_6 - fitness_min_6) / 2
fitness_std_7 = (fitness_max_7 - fitness_min_7) / 2
fitness_std_8 = (fitness_max_8 - fitness_min_8) / 2

# Plotting the average fitness with variance (as shaded regions)
plt.figure(figsize=(12, 8))


for i, (fitness_avg, fitness_std) in enumerate(
    [(fitness_avg_0, fitness_std_0),
     (fitness_avg_1, fitness_std_1),
     (fitness_avg_2, fitness_std_2),
     (fitness_avg_3, fitness_std_3),
     (fitness_avg_4, fitness_std_4),
     (fitness_avg_5, fitness_std_5),
     (fitness_avg_6, fitness_std_6),
     (fitness_avg_7, fitness_std_7),
     (fitness_avg_8, fitness_std_8),
 ]  # Include the sixth partition
):
    plt.plot(generations, fitness_avg, marker='o', linestyle='-', label=f'Partition {i} Avg Fitness')

# Customizing the plot for research-level presentation
plt.title('Progression of Average Fitness Scores with Variance across Generations')
plt.xlabel('Generation')
plt.ylabel('Average Fitness Score')
plt.grid(True)
plt.legend(loc='lower right')
plt.ylim([0.7, 0.77])  # Adjusting the y-limit based on the provided fitness values

# Save the plot for publication
plt.savefig('research_level_fitness_progression.png', dpi=300, bbox_inches='tight')

plt.show()